#!/usr/bin/env python3
"""
æ£€æŸ¥ä¸åŒbatch_sizeçš„æ˜¾å­˜å ç”¨æƒ…å†µ
"""

import torch
import gc
import psutil
import os
from efficient_sam.build_efficient_sam import build_efficient_sam_vitt


def get_memory_usage():
    """è·å–å½“å‰æ˜¾å­˜ä½¿ç”¨æƒ…å†µ"""
    if torch.cuda.is_available():
        return {
            'allocated': torch.cuda.memory_allocated() / 1024**3,  # GB
            'cached': torch.cuda.memory_reserved() / 1024**3,  # GB
            'max_allocated': torch.cuda.max_memory_allocated() / 1024**3  # GB
        }
    else:
        return {'error': 'CUDA not available'}


def test_batch_sizes():
    """æµ‹è¯•ä¸åŒbatch_sizeçš„æ˜¾å­˜å ç”¨"""
    print("ğŸ” æµ‹è¯•ä¸åŒbatch_sizeçš„æ˜¾å­˜å ç”¨...")

    # åˆå§‹åŒ–æ¨¡å‹
    model = build_efficient_sam_vitt()
    model = model.cuda()

    # æµ‹è¯•ä¸åŒçš„batch_size
    batch_sizes = [1, 2, 4, 8, 16]
    image_shape = (3, 1024, 1024)  # C, H, W

    results = []

    for batch_size in batch_sizes:
        print(f"\nğŸ“Š æµ‹è¯• batch_size = {batch_size}")

        # æ¸…ç†æ˜¾å­˜
        torch.cuda.empty_cache()
        gc.collect()

        # è·å–åˆå§‹æ˜¾å­˜
        initial_memory = get_memory_usage()
        print(f"   åˆå§‹æ˜¾å­˜: {initial_memory.get('allocated', 0):.2f} GB")

        try:
            # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®
            images = torch.randn(batch_size, *image_shape).cuda()
            point_prompts = torch.randn(batch_size, 10, 4, 2).cuda()  # max_objects=10
            point_labels = torch.randint(0, 4, (batch_size, 10, 4)).cuda()

            # å‰å‘ä¼ æ’­
            with torch.no_grad():
                image_embeddings = model.get_image_embeddings(images)
                masks, ious = model.predict_masks(
                    image_embeddings=image_embeddings,
                    batched_points=point_prompts,
                    batched_point_labels=point_labels,
                    multimask_output=True,
                    input_h=1024,
                    input_w=1024,
                    output_h=1024,
                    output_w=1024
                )

            # è·å–æœ€ç»ˆæ˜¾å­˜
            final_memory = get_memory_usage()
            allocated = final_memory.get('allocated', 0)
            initial = initial_memory.get('allocated', 0)

            # è®¡ç®—å¢é‡
            memory_increase = allocated - initial

            result = {
                'batch_size': batch_size,
                'initial_memory_gb': initial,
                'final_memory_gb': allocated,
                'increase_gb': memory_increase,
                'increase_per_sample': memory_increase / batch_size if batch_size > 0 else 0
            }

            results.append(result)

            print(f"   æœ€ç»ˆæ˜¾å­˜: {allocated:.2f} GB")
            print(f"   æ˜¾å­˜å¢åŠ : {memory_increase:.2f} GB")
            print(f"   æ¯æ ·æœ¬å¢åŠ : {memory_increase / batch_size if batch_size > 0 else 0:.3f} GB")

            # åˆ é™¤å¼ é‡
            del images, point_prompts, point_labels, image_embeddings, masks, ious

        except torch.cuda.OutOfMemoryError:
            print(f"   âŒ æ˜¾å­˜ä¸è¶³ï¼batch_size {batch_size} å¤ªå¤§")
            results.append({
                'batch_size': batch_size,
                'error': 'Out of Memory'
            })
        except Exception as e:
            print(f"   âŒ é”™è¯¯: {e}")
            results.append({
                'batch_size': batch_size,
                'error': str(e)
            })

    # æ‰“å°æ€»ç»“
    print("\n" + "="*60)
    print("ğŸ“Š æ˜¾å­˜ä½¿ç”¨æ€»ç»“:")
    print("="*60)

    print(f"{'Batch Size':<12} {'æ˜¾å­˜å¢åŠ (GB)':<15} {'æ¯æ ·æœ¬(GB)':<12}")
    print("-" * 40)

    for result in results:
        if 'error' not in result:
            bs = result['batch_size']
            inc = result['increase_gb']
            per_sample = result['increase_per_sample']
            print(f"{bs:<12} {inc:<15.3f} {per_sample:<12.3f}")
        else:
            bs = result['batch_size']
            error = result['error']
            print(f"{bs:<12} {error:<15}")

    return results


def analyze_actual_batch_usage():
    """åˆ†æå®é™…çš„batchä½¿ç”¨æƒ…å†µ"""
    print("\nğŸ” åˆ†æå®é™…çš„batchä½¿ç”¨æƒ…å†µ...")

    # æ£€æŸ¥æ•°æ®é›†
    print("1. æ•°æ®é›†é…ç½®:")
    print("   - max_objects:", 10)  # ä»é…ç½®ä¸­è¯»å–
    print("   - å›¾åƒå°ºå¯¸: 1024x1024")

    # è®¡ç®—ç†è®ºæ˜¾å­˜
    print("\n2. ç†è®ºæ˜¾å­˜è®¡ç®—:")

    # æ¨¡å‹å‚æ•°
    model_params = 12e6  # 12Må‚æ•°
    param_memory = model_params * 4 / 1024**3  # float32 = 4å­—èŠ‚
    print(f"   æ¨¡å‹å‚æ•°: {param_memory:.2f} GB")

    # æ¿€æ´»å€¼ (ç²—ç•¥ä¼°è®¡)
    batch_size = 8
    channels = 256  # ä¸­é—´å±‚é€šé“æ•°
    activation_memory = batch_size * channels * 1024 * 1024 * 4 / 1024**3
    print(f"   æ¿€æ´»å€¼ (ä¼°è®¡): {activation_memory:.2f} GB")

    # masksæ˜¾å­˜
    masks_memory = batch_size * 10 * 1024 * 1024 * 4 / 1024**3  # max_objects=10
    print(f"   masksæ˜¾å­˜: {masks_memory:.2f} GB")

    total_estimate = param_memory + activation_memory + masks_memory
    print(f"   æ€»ä¼°è®¡: {total_estimate:.2f} GB")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ Batch Size æ˜¾å­˜åˆ†æå·¥å…·")
    print("=" * 50)

    # æ£€æŸ¥CUDAå¯ç”¨æ€§
    if not torch.cuda.is_available():
        print("âŒ CUDAä¸å¯ç”¨")
        return

    print(f"âœ… CUDAå¯ç”¨")
    print(f"   è®¾å¤‡: {torch.cuda.get_device_name(0)}")
    print(f"   æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")

    # åˆ†æç†è®ºä½¿ç”¨
    analyze_actual_batch_usage()

    # æµ‹è¯•å®é™…ä½¿ç”¨
    print("\nğŸ§ª å¼€å§‹å®é™…æµ‹è¯•...")
    results = test_batch_sizes()

    # å»ºè®®
    print("\nğŸ’¡ å»ºè®®:")
    print("1. å¦‚æœæ˜¾å­˜å¢åŠ ä¸æ˜æ˜¾ï¼Œå¯èƒ½æ˜¯ï¼š")
    print("   - PyTorché¢„åˆ†é…äº†è¶³å¤Ÿæ˜¾å­˜")
    print("   - ä¸»è¦æ˜¾å­˜æ¶ˆè€—åœ¨å…¶ä»–åœ°æ–¹ï¼ˆå¦‚ä¸­é—´å˜é‡ï¼‰")
    print("   - ä½¿ç”¨äº†æ¢¯åº¦ç´¯ç§¯ç­‰ä¼˜åŒ–æŠ€æœ¯")

    print("2. ä¼˜åŒ–å»ºè®®ï¼š")
    print("   - ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ (amp)")
    print("   - å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ (gradient checkpointing)")
    print("   - è°ƒæ•´æ•°æ®åŠ è½½å™¨çš„num_workers")
    print("   - ä½¿ç”¨æ›´å°çš„å›¾åƒå°ºå¯¸è¿›è¡Œé¢„å®éªŒ")


if __name__ == '__main__':
    main()