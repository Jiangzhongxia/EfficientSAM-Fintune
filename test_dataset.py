#!/usr/bin/env python3
# Copyright (c) Meta Platforms, Inc. and affiliates.
# All rights reserved.

# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.

"""
æµ‹è¯•ä¿®å¤åçš„COCOæ•°æ®é›†åŠ è½½å™¨
"""

import os
import sys
import json
import torch
from torch.utils.data import DataLoader
from coco_dataset import COCODataset, get_coco_transforms, collate_fn


def create_test_annotation():
    """åˆ›å»ºæµ‹è¯•ç”¨çš„COCOæ ‡æ³¨æ–‡ä»¶"""
    # åˆ›å»ºæµ‹è¯•æ ‡æ³¨
    test_annotation = {
        "images": [
            {
                "id": 1,
                "width": 512,
                "height": 512,
                "file_name": "test_image.jpg"
            }
        ],
        "annotations": [
            {
                "id": 1,
                "image_id": 1,
                "category_id": 1,
                "segmentation": [[100, 100, 200, 100, 200, 200, 100, 200]],  # ç®€å•æ­£æ–¹å½¢
                "area": 10000,
                "bbox": [100, 100, 100, 100],  # x, y, w, h
                "iscrowd": 0
            },
            {
                "id": 2,
                "image_id": 1,
                "category_id": 1,
                "segmentation": [[300, 300, 400, 300, 400, 400, 300, 400]],  # å¦ä¸€ä¸ªæ­£æ–¹å½¢
                "area": 10000,
                "bbox": [300, 300, 100, 100],  # x, y, w, h
                "iscrowd": 0
            }
        ]
    }

    # ä¿å­˜æµ‹è¯•æ ‡æ³¨æ–‡ä»¶
    with open('test_annotation.json', 'w') as f:
        json.dump(test_annotation, f, indent=2)

    print("âœ… åˆ›å»ºæµ‹è¯•æ ‡æ³¨æ–‡ä»¶: test_annotation.json")


def create_test_image():
    """åˆ›å»ºæµ‹è¯•ç”¨çš„å›¾åƒæ–‡ä»¶"""
    from PIL import Image
    import numpy as np

    # åˆ›å»ºä¸€ä¸ªç®€å•çš„æµ‹è¯•å›¾åƒ
    img_array = np.random.randint(0, 255, (512, 512, 3), dtype=np.uint8)
    img = Image.fromarray(img_array)
    img.save('test_image.jpg')

    print("âœ… åˆ›å»ºæµ‹è¯•å›¾åƒ: test_image.jpg")


def test_dataset_loading():
    """æµ‹è¯•æ•°æ®é›†åŠ è½½"""
    print("\nğŸ” æµ‹è¯•æ•°æ®é›†åŠ è½½...")

    try:
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        create_test_annotation()
        create_test_image()

        # åˆå§‹åŒ–æ•°æ®é›†
        dataset = COCODataset(
            root_dir=".",  # å½“å‰ç›®å½•
            annotation_file="test_annotation.json",
            transform=get_coco_transforms(1024),
            target_size=1024,
            max_objects=10,
            iou_threshold=0.5,
            random_box_augmentation=False,
            box_noise_scale=0.0
        )

        print(f"âœ… æ•°æ®é›†åˆå§‹åŒ–æˆåŠŸï¼ŒåŒ…å« {len(dataset)} ä¸ªæ ·æœ¬")

        # æµ‹è¯•è·å–å•ä¸ªæ ·æœ¬
        print("\nğŸ” æµ‹è¯•è·å–å•ä¸ªæ ·æœ¬...")
        sample = dataset[0]

        # æ£€æŸ¥æ ·æœ¬å†…å®¹
        required_keys = ['image', 'original_size', 'boxes', 'point_prompts', 'point_labels', 'masks', 'num_objects']

        for key in required_keys:
            if key not in sample:
                print(f"âŒ ç¼ºå°‘å…³é”®é”®: {key}")
                return False
            else:
                print(f"âœ… æ‰¾åˆ°é”®: {key}, å½¢çŠ¶: {sample[key].shape if hasattr(sample[key], 'shape') else type(sample[key])}")

        # æ£€æŸ¥æ•°æ®ç±»å‹å’Œå½¢çŠ¶
        image = sample['image']
        if not isinstance(image, torch.Tensor) or image.shape != (3, 1024, 1024):
            print(f"âŒ å›¾åƒæ ¼å¼é”™è¯¯: {type(image)}, å½¢çŠ¶: {image.shape}")
            return False

        boxes = sample['boxes']
        if not isinstance(boxes, torch.Tensor):
            print(f"âŒ boxesæ ¼å¼é”™è¯¯: {type(boxes)}")
            return False

        point_prompts = sample['point_prompts']
        if not isinstance(point_prompts, torch.Tensor):
            print(f"âŒ point_promptsæ ¼å¼é”™è¯¯: {type(point_prompts)}")
            return False

        masks = sample['masks']
        if not isinstance(masks, torch.Tensor):
            print(f"âŒ masksæ ¼å¼é”™è¯¯: {type(masks)}")
            return False

        num_objects = sample['num_objects']
        if num_objects > 0:
            print(f"âœ… æ£€æµ‹åˆ° {num_objects} ä¸ªæœ‰æ•ˆç‰©ä½“")
        else:
            print("âš ï¸  æ²¡æœ‰æ£€æµ‹åˆ°æœ‰æ•ˆç‰©ä½“")

        return True

    except Exception as e:
        print(f"âŒ æ•°æ®é›†åŠ è½½æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_dataloader():
    """æµ‹è¯•æ•°æ®åŠ è½½å™¨"""
    print("\nğŸ” æµ‹è¯•æ•°æ®åŠ è½½å™¨...")

    try:
        # åˆå§‹åŒ–æ•°æ®é›†
        dataset = COCODataset(
            root_dir=".",
            annotation_file="test_annotation.json",
            transform=get_coco_transforms(1024),
            target_size=1024,
            max_objects=10,
            iou_threshold=0.5,
            random_box_augmentation=False,
            box_noise_scale=0.0
        )

        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        dataloader = DataLoader(
            dataset,
            batch_size=2,
            shuffle=False,
            num_workers=0,  # ä½¿ç”¨ä¸»è¿›ç¨‹ï¼Œé¿å…å¤šè¿›ç¨‹é—®é¢˜
            collate_fn=collate_fn
        )

        print("âœ… æ•°æ®åŠ è½½å™¨åˆ›å»ºæˆåŠŸ")

        # æµ‹è¯•æ‰¹é‡åŠ è½½
        print("\nğŸ” æµ‹è¯•æ‰¹é‡åŠ è½½...")
        batch = next(iter(dataloader))

        # æ£€æŸ¥batchå†…å®¹
        required_keys = ['images', 'original_sizes', 'point_prompts', 'point_labels', 'masks', 'num_objects']

        for key in required_keys:
            if key not in batch:
                print(f"âŒ batchç¼ºå°‘å…³é”®é”®: {key}")
                return False
            else:
                print(f"âœ… batchæ‰¾åˆ°é”®: {key}, å½¢çŠ¶: {batch[key].shape}")

        # æ£€æŸ¥batchæ•°æ®å½¢çŠ¶
        batch_size = len(dataset)
        if batch['images'].shape[0] != batch_size:
            print(f"âŒ batch sizeä¸åŒ¹é…: {batch['images'].shape[0]} != {batch_size}")
            return False

        print(f"âœ… æ‰¹é‡åŠ è½½æµ‹è¯•æˆåŠŸï¼Œbatch size: {batch_size}")

        return True

    except Exception as e:
        print(f"âŒ æ•°æ®åŠ è½½å™¨æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def cleanup_test_files():
    """æ¸…ç†æµ‹è¯•æ–‡ä»¶"""
    test_files = ['test_annotation.json', 'test_image.jpg']
    for file in test_files:
        if os.path.exists(file):
            os.remove(file)
            print(f"âœ… åˆ é™¤æµ‹è¯•æ–‡ä»¶: {file}")


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ COCOæ•°æ®é›†åŠ è½½å™¨æµ‹è¯•")
    print("=" * 50)

    tests = [
        ("æ•°æ®é›†åŠ è½½", test_dataset_loading),
        ("æ•°æ®åŠ è½½å™¨", test_dataloader),
    ]

    results = []
    for test_name, test_func in tests:
        try:
            result = test_func()
            results.append(result)
        except Exception as e:
            print(f"âŒ {test_name} æµ‹è¯•å‡ºç°å¼‚å¸¸: {e}")
            results.append(False)

    # æ¸…ç†æµ‹è¯•æ–‡ä»¶
    cleanup_test_files()

    # æ€»ç»“
    print("\n" + "=" * 50)
    print("ğŸ“Š æµ‹è¯•ç»“æœæ€»ç»“:")
    for i, (test_name, _) in enumerate(tests):
        status = "âœ… é€šè¿‡" if results[i] else "âŒ å¤±è´¥"
        print(f"   {test_name}: {status}")

    passed = sum(results)
    total = len(results)
    print(f"\nğŸ¯ æ€»ä½“ç»“æœ: {passed}/{total} æµ‹è¯•é€šè¿‡")

    if passed == total:
        print("ğŸ‰ æ­å–œï¼æ•°æ®é›†åŠ è½½å™¨ä¿®å¤æˆåŠŸï¼Œå¯ä»¥å¼€å§‹è®­ç»ƒï¼")
        return 0
    else:
        print("âš ï¸  å­˜åœ¨é—®é¢˜ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")
        return 1


if __name__ == '__main__':
    exit_code = main()
    sys.exit(exit_code)